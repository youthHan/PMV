<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <meta name="description" content="Portrait-Mode Video Recognition">
  <meta name="keywords"
    content="Portrait-Mode, Video Recognition, Benchmark, Temporal Distinct, Vertical Videos, Portrait Mode Videos, Action Recognition, Dataset, Portrait-mode Videos, Vertical-orientation, Vertical-orientation Videos">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Video Recognition in Portrait Mode</title>

  <!-- Global site tag (gtag.js) - Google Analytics
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>


<style>
  .image-container {
    display: flex;
    justify-content: center; /* Centers the images horizontally */
    align-items: center; /* Align items vertically */
  }
  
  .image-container img {
    margin-right: 15px; /* Adds spacing between the images */
  }
  
  /* For the portrait image (9:16) */
  .image-one {
    width: 500px; /* Width is specified for landscape to determine its size, calculated from the portrait height for a similar aspect ratio visibility */
    height: auto; /* Height auto for aspect ratio */
  
  }
  
  /* For the landscape image (16:9) */
  .image-two {
    height: 228px; /* Height is specified for portrait to determine its size */
    width: auto; /* Width auto for aspect ratio */
    }
  /* For the landscape image (16:9) */
  .image-acc {
    height: 170px; /* Height is specified for portrait to determine its size */
    width: auto; /* Width auto for aspect ratio */
    }
  
  /* Removes margin from the last image to avoid extra spacing on the right */
  .image-container img:last-child {
    margin-right: 0;
  }
  
</style>

<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://mingfei.info">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://roomtour3d.github.io">
            RoomTour3D
          </a>
          <a class="navbar-item" href="https://mingfei.info/shot2story/">
            Shot2Story
          </a>
          <a class="navbar-item" href="https://mingfei.info/PMV/">
            Portrait-mode Videos
          </a>
          <a class="navbar-item" href="https://github.com/ziplab/LongVLM">
            LongVLM for videos
          </a>
        </div>
      </div>
    </div>

  </div>
</nav>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">Video Recognition in Portrait Mode</h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a href="https://mingfei.info">Mingfei Han<sup>2,1,3,4</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://sites.google.com/site/linjieyang89/">Linjie Yang<sup>1</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://airobotai.github.io/jinxiaojie/">Xiaojie Jin<sup>1</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://sites.google.com/site/jshfeng/home">Jiashi Feng<sup>1</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://www.xiaojun.ai/">Xiaojun Chang<sup>2,4</sup></a>,
              </span>
              <span class="author-block">
                <a href="https://hengcv.github.io/">Heng Wang<sup>1</sup></a>
              </span>
            </div>

            <div class="is-size-5 publication-authors">
              <span class="author-block"><sup>1</sup>Bytedance Inc.,</span>
              <span class="author-block"><sup>2</sup>ReLER, AAII, University of Technology Sydney,</span>
              <span class="author-block"><sup>3</sup>Data61, CSIRO,</span>
              <span class="author-block"><sup>4</sup>Department of Computer Vision, MBZUAI</span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <!-- PDF Link. -->
                <span class="link-block">
                  <a href="https://arxiv.org/pdf/2312.13746"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                    </span>
                    <span>Paper</span>
                  </a>
                </span>
                <span class="link-block">
                  <a href="https://arxiv.org/abs/2312.13746"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="ai ai-arxiv"></i>
                    </span>
                    <span>CVPR2024</span>
                  </a>
                </span>
                <!-- Code Link. -->
                <span class="link-block">
                  <a href="https://github.com/bytedance/Portrait-Mode-Video"
                     class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                        <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                    </a>
                </span>
                <!-- Dataset Link. -->
                <span class="link-block">
                  <a href="https://github.com/bytedance/Portrait-Mode-Video/blob/master/DATA.md"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="far fa-images"></i>
                    </span>
                    <span>Data</span>
                  </a>
              </div>

            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <h2 class="subtitle has-text-centered">
          We have developed the first dataset dedicated to portrait mode video recognition, namely PortraitMode-400 and
          focus on the research of this emerging video format.
        </h2>
      </div>
    </div>
  </section>

  <section class="hero is-light is-small">
    <div class="hero-body">
      <div class="container">
        <div id="results-carousel" class="carousel results-carousel">
          <div class="item item-huajuan">
            <video poster="" id="huajuan" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/v0d00fg10000cceqgb3c77ub16cnepb0.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-paint2">
            <video poster="" id="paint2" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/v0d00fg10000c4mo34bc77u339ves21g.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-cast-net">
            <video poster="" id="cast-net" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/v0d00fg10000cb5tpjjc77u7h45duqa0.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-firework">
            <video poster="" id="firework" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/v0200fg10000c2hqshct1rmi4vupplog.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-yoga">
            <video poster="" id="yoga" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/v0200fg10000c5r571rc77u4a7dmkc30.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-latte-art">
            <video poster="" id="latte-art" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/v0d00cg10000c3hut6rc77ucgf3rn5tg.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-paint1">
            <video poster="" id="paint1" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/v0200fg10000c9iheb3c77u7bg1k1ok0.mp4" type="video/mp4">
            </video>
          </div>
          <div class="item item-cross-stitch">
            <video poster="" id="cross-stitch" autoplay controls muted loop playsinline height="100%">
              <source src="./static/videos/v0200fg10000c98djubc77ufen5979ug.mp4" type="video/mp4">
            </video>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <!-- Abstract. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p>
              The creation of new datasets often presents new challenges for video recognition and can inspire novel
              ideas while addressing these challenges. While existing datasets mainly comprise landscape mode videos,
              our paper seeks to introduce portrait mode videos to the research community and highlight the unique
              challenges associated with this video format. With the growing popularity of smartphones and social
              media applications, recognizing portrait mode videos is becoming increasingly important.
            </p>
            <p>To this end, we have developed the first dataset dedicated to portrait mode video recognition, namely
              PortraitMode-400.
              The taxonomy of PortraitMode-400 was constructed in a data-driven manner, comprising 400 fine-grained
              categories, and rigorous quality assurance was implemented to ensure the accuracy of human annotations.
            </p>
            <p>
              In addition to the new dataset, we conducted a comprehensive analysis of the impact of video format
              (portrait mode versus landscape mode) on recognition accuracy and spatial bias due to the different
              formats.
            </p>
            <p>Furthermore, we designed extensive experiments to explore key aspects of portrait mode video
              recognition, including the choice of data augmentation and evaluation procedure. Building on the
              insights from our experimental results and the introduction of PortraitMode-400, our paper aims to
              inspire further research efforts in this emerging research area.
            </p>
          </div>
        </div>
      </div>
      </br>
      </br>
      <!--/ Abstract. -->

      <!-- Contributions. -->
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">PortraitMode-400</h2>
          <div class="content has-text-justified">
            <p>
              While existing video datasets are mostly built on landscape mode videos, portrait mode videos have become
              increasingly more popular on major social media applications. The shift from landscape mode to portrait
              mode is not just changing the aspect ratios of the videos. It has significant implications for the types
              of content that are created and the spatial bias inherent in the data. 
            </p>
            <p>
              Portrait mode videos bring in distinct challenges for video recognition as well. For example, they tend to focus more on the subject (i.e., typically humans) with much less background context, and include more egocentric content. In addition, they contain a lot of verbal communication that is essential to understand the video content. There is a pressing need for portrait mode video datasets to explore these new research problems.
            </p>
            <p>
              To facilitate the research in portrait mode videos, we introduce the first dataset dedicated to portrait mode video recognition, named PotraitMode-400. Some demo videos are shown above.
            </p>
          </div>
          </br>
          </br>
          <h2 class="title is-3">Taxonomy</h2>
          Click on each of the parent nodes to view an overview of its sub-categoris.
          <figure>
            <iframe src="./PMV_400_taxonomy.html" width="600" height="600"></iframe>
            <!-- <figcaption>Our taxonomy contains 400 diverse action-focused categories, organized in a hierarchical structure. </figcaption> -->
          </figure>
          </br>
          <div class="content has-text-justified">
            <p>
              The PortraitMode-400 dataset was developed using a unique taxonomy created from Douyin video search queries, focusing on action-oriented content. Unlike existing datasets, which often repurpose categories, PortraitMode-400â€™s taxonomy is crafted from the ground up, analyzing around 38,000 queries to select approximately 2,400 with clear action or motion, leading to about 500 distinct, action-focused categories arranged in a hierarchical structure. This method ensures comprehensive coverage of diverse content types, including daily activities, natural events, and transport scenes, with a notable depth in categories, such as 89 distinct types under food alone. This approach offers a more detailed and relevant classification system for portrait mode videos, diverging from broader classifications seen in datasets like Kinetics-400 or 3Massiv.
            </p>
          </div>
          </br>
          </br>
          <h2 class="title is-3">Portrait Mode vs. Landscape Mode</h2>
          <div class="content has-text-justified">
            <div class="image-container">
              <img src="./static/images/LM_prob.png" alt="First Image" class="image-one">
              <img src="./static/images/PM_prob.png" alt="Second Image" class="image-two">
            </div>   
            </br>
            <p>
              <strong>Question: </strong>How are the portrait mode video different from landscape modes videos?
            </p>
            <p>
              <strong>Answer: </strong>We show the different data priors of videos in different display mode by the above accuracy difference map. 
              The first thing to emphasize is that it is not just an aspect ratio change between PM videos and LM videos. It include different topic selection, shooting approach and content arrangement.
              To visualize such difference, we train models with S100-PM and S100-LM seperately and test them exhaustively on local spatial segments of either S100-PM test or S100-LM test videos, and make difference.
              Our findings demonstrate that models trained on LM and PM videos exhibit proficiency in different spatial regions of the videos corresponding to their training data. This reveals the distinct data biases inherent in LM and PM videos.
            </p>        
            <!-- <p>
              <strong>Question: </strong>How well does a model trained on landscape mode videos perform on portrait mode
              videos, and vice versa?
            </p>
            <p>
              <strong>Answer: </strong>We investigate this question by constructing a subset from the Kinetics-700
              dataset for a
              rigorous comparison and visualize classification heatmaps to reveal the differences in spatial bias
              resulting from the change in video format.
            </p> -->
          </div>
          </br>
          </br>
          <h2 class="title is-3">Optimal protocols</h2>
          <div class="content has-text-justified">
            <div class="image-container">
              <img src="./static/images/figure3.png" alt="First Image" class="image-acc">
              <img src="./static/images/figure4.png" alt="Second Image" class="image-acc">
              <img src="./static/images/figure5.png" alt="Second Image" class="image-acc">
            </div>   
            </br>
            <p>
              <strong>Question: </strong>What are the optimal training and testing protocols for portrait mode video
              recognition?
            </p>
            <p>
              <strong>Answer: </strong>We delve into various components of state-of-the-art deep learning systems, such
              as data augmentation,
              evaluation cropping strategies, etc. Some of our findings contradict the current standard practices
              for landscape mode videos, highlighting the need for further research in the domain of portrait mode
              videos.
            </p>
          </div>
        </div>
      </div>
      <!--/ Contributions. -->


    </div>
  </section>


  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>
        @misc{han2023pmv,
          title={Video Recognition in Portrait Mode}, 
          author={Mingfei Han and Linjie Yang and Xiaojie Jin and Jiashi Feng and Xiaojun Chang and Heng Wang},
          year={2023},
          eprint={2312.13746},
          archivePrefix={arXiv},
          primaryClass={cs.CV}
        }
  </code></pre>
    </div>
  </section>


  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              Website design from <a href="https://nerfies.github.io/">Nerfies</a>.
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>
